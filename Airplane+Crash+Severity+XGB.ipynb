{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    import pandas as pd\n",
    "\n",
    "    train = pd.read_csv(r\"F:\\Machine_learning\\HackerEarth\\train.csv\")\n",
    "    test = pd.read_csv(r\"F:\\Machine_learning\\HackerEarth\\test.csv\")\n",
    "\n",
    "    train.columns\n",
    "    train.dtypes\n",
    "    # ALL features are either float or integers. and the output is categorical into 4 values. which will be mapped to 1,2,3,4\n",
    "    train.Violations.value_counts() # this is categorical Variable\n",
    "    train.Accident_Type_Code.value_counts() # This is a Categorical variable\n",
    "\n",
    "\n",
    "    train = pd.get_dummies(train, columns =['Accident_Type_Code','Violations'])\n",
    "    test = pd.get_dummies(test, columns =['Accident_Type_Code','Violations'])\n",
    "\n",
    "    train.columns\n",
    "    train = train[['Accident_ID', 'Safety_Score', 'Days_Since_Inspection',\n",
    "           'Total_Safety_Complaints', 'Control_Metric', 'Turbulence_In_gforces',\n",
    "           'Cabin_Temperature', 'Max_Elevation', 'Adverse_Weather_Metric',\n",
    "           'Accident_Type_Code_1', 'Accident_Type_Code_2',\n",
    "           'Accident_Type_Code_3', 'Accident_Type_Code_4', 'Accident_Type_Code_5',\n",
    "           'Accident_Type_Code_6', 'Accident_Type_Code_7', 'Violations_0',\n",
    "           'Violations_1', 'Violations_2', 'Violations_3', 'Violations_4',\n",
    "           'Violations_5','Severity']]\n",
    "    # 'Severity' is sent to last columns as that is the output. Accident_ID is made first columns since thats the output format.\n",
    "\n",
    "    train.Severity.value_counts().sum() # 10000 values\n",
    "    train.shape # 10000 x 23\n",
    "    # to confirm the values in the output and shape of the train data.\n",
    "    train.Severity.value_counts() # Distribution of the data along output types.\n",
    "\n",
    "    # Mapping all 4 types to numeric values. \n",
    "    train.Severity = train.Severity.map({'Highly_Fatal_And_Damaging':0,\n",
    "                                         'Significant_Damage_And_Serious_Injuries':1,\n",
    "                                         'Minor_Damage_And_Injuries':2,\n",
    "                                         'Significant_Damage_And_Fatalities':3},)\n",
    "    test.columns\n",
    "    # Adding Accident_ID as the first column to the test data.\n",
    "    test = test[['Accident_ID','Safety_Score', 'Days_Since_Inspection', 'Total_Safety_Complaints',\n",
    "           'Control_Metric', 'Turbulence_In_gforces', 'Cabin_Temperature',\n",
    "           'Max_Elevation', 'Adverse_Weather_Metric',\n",
    "           'Accident_Type_Code_1', 'Accident_Type_Code_2', 'Accident_Type_Code_3',\n",
    "           'Accident_Type_Code_4', 'Accident_Type_Code_5', 'Accident_Type_Code_6',\n",
    "           'Accident_Type_Code_7', 'Violations_0', 'Violations_1', 'Violations_2',\n",
    "           'Violations_3', 'Violations_4', 'Violations_5']]\n",
    "\n",
    "    train.shape, test.shape # one extra column in train data which is the output.\n",
    "    # since ID's are not the part of the data input, they are saved into different columns and later will mapped onto\n",
    "    # the output data for submission.\n",
    "    Acc_ID_Train = train.Accident_ID\n",
    "    Acc_ID_Test = test.Accident_ID\n",
    "    # Dropping the Accident ID\n",
    "    train = train.iloc[:,1:]\n",
    "    test = test.iloc[:,1:]\n",
    "\n",
    "\n",
    "    # Create X_train, y_train and we have test data withour output.\n",
    "    X = train.iloc[:,0:-1]\n",
    "    y = train.iloc[:,[-1]] # severityb\n",
    "\n",
    "    # X.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spens\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\spens\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting XGBoost to the Training set\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier(objective='multi:softmax')\n",
    "# objective='multi:softmax' for multiclass classifier.\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[262,   7,  32,   8],\n",
       "       [ 14, 244,  36,   5],\n",
       "       [ 11,  13, 202,  17],\n",
       "       [ 10,   9,  14, 116]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.399999999999991"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model, metrics \n",
    "metrics.accuracy_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc.transform(test)\n",
    "y_pred_sub = classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(Acc_ID_Test)\n",
    "sub['Severity'] = y_pred_sub.tolist()\n",
    "sub.Severity = sub.Severity.map({0:'Highly_Fatal_And_Damaging',\n",
    "                   1:'Significant_Damage_And_Serious_Injuries',\n",
    "                   2:'Minor_Damage_And_Injuries',\n",
    "                   3:'Significant_Damage_And_Fatalities'})\n",
    "sub.to_csv(r\"F:\\Machine_learning\\HackerEarth\\sample_submission_xgb.csv\",index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
